{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "from itertools import groupby\n",
    "from tictactoe import TicTacToeGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, game_class, epsilon=0.1, alpha=0.5, value_player='X'):\n",
    "        self.V = dict()\n",
    "        self.NewGame = game_class\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.value_player = value_player\n",
    "\n",
    "    def state_value(self, game_state):\n",
    "        return self.V.get(game_state, 0.0)\n",
    "\n",
    "    def learn_game(self, num_episodes=1000):\n",
    "        for episode in range(num_episodes):\n",
    "            self.learn_from_episode()\n",
    "\n",
    "    def learn_from_episode(self):\n",
    "        game = self.NewGame()\n",
    "        _, move = self.learn_select_move(game)\n",
    "        while move:\n",
    "            move = self.learn_from_move(game, move)\n",
    "\n",
    "    def learn_from_move(self, game, move):\n",
    "        game.make_move(move)\n",
    "        r = self.__reward(game)\n",
    "        td_target = r\n",
    "        next_state_value = 0.0\n",
    "        selected_next_move = None\n",
    "        if game.playable():\n",
    "            best_next_move, selected_next_move = self.learn_select_move(game)\n",
    "            next_state_value = self.state_value(best_next_move)\n",
    "        current_state_value = self.state_value(move)\n",
    "        td_target = r + next_state_value\n",
    "        self.V[move] = current_state_value + self.alpha * (td_target - current_state_value)\n",
    "        return selected_next_move\n",
    "\n",
    "    def learn_select_move(self, game):\n",
    "        allowed_state_values = self.__state_values( game.allowed_moves() )\n",
    "        if game.player == self.value_player:\n",
    "            best_move = self.__argmax_V(allowed_state_values)\n",
    "        else:\n",
    "            best_move = self.__argmin_V(allowed_state_values)\n",
    "\n",
    "        selected_move = best_move\n",
    "        if random.random() < self.epsilon:\n",
    "            selected_move = self.__random_V(allowed_state_values)\n",
    "\n",
    "        return (best_move, selected_move)\n",
    "\n",
    "    def play_select_move(self, game):\n",
    "        allowed_state_values = self.__state_values( game.allowed_moves() )\n",
    "        if game.player == self.value_player:\n",
    "            return self.__argmax_V(allowed_state_values)\n",
    "        else:\n",
    "            return self.__argmin_V(allowed_state_values)\n",
    "\n",
    "    def demo_game(self, verbose=False):\n",
    "        game = self.NewGame()\n",
    "        t = 0\n",
    "        while game.playable():\n",
    "            if verbose:\n",
    "                print(\" \\nTurn {}\\n\".format(t))\n",
    "                game.print_board()\n",
    "            move = self.play_select_move(game)\n",
    "            game.make_move(move)\n",
    "            t += 1\n",
    "        if verbose:\n",
    "            print(\" \\nTurn {}\\n\".format(t))\n",
    "            game.print_board()\n",
    "        if game.winner:\n",
    "            if verbose:\n",
    "                print(\"\\n{} is the winner!\".format(game.winner))\n",
    "            return game.winner\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"\\nIt's a draw!\")\n",
    "            return '-'\n",
    "\n",
    "    def interactive_game(self, agent_player='X'):\n",
    "        game = self.NewGame()\n",
    "        t = 0\n",
    "        while game.playable():\n",
    "            print(\" \\nTurn {}\\n\".format(t))\n",
    "            game.print_board()\n",
    "            if game.player == agent_player:\n",
    "                move = self.play_select_move(game)\n",
    "                game.make_move(move)\n",
    "            else:\n",
    "                move = self.__request_human_move(game)\n",
    "                game.v(move)\n",
    "            t += 1\n",
    "\n",
    "        print(\" \\nTurn {}\\n\".format(t))\n",
    "        game.print_board()\n",
    "\n",
    "        if game.winner:\n",
    "            print(\"\\n{} is the winner!\".format(game.winner))\n",
    "            return game.winner\n",
    "        print(\"\\nIt's a draw!\")\n",
    "        return '-'\n",
    "\n",
    "    def round_V(self):\n",
    "        # After training, this makes action selection random from equally-good choices\n",
    "        for k in self.V.keys():\n",
    "            self.V[k] = round(self.V[k],1)\n",
    "\n",
    "    def save_v_table(self):\n",
    "        with open('state_values.csv', 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['State', 'Value'])\n",
    "            all_states = list(self.V.keys())\n",
    "            all_states.sort()\n",
    "            for state in all_states:\n",
    "                writer.writerow([state, self.V[state]])\n",
    "\n",
    "    def __state_values(self, game_states):\n",
    "        return dict((state, self.state_value(state)) for state in game_states)\n",
    "\n",
    "    def __argmax_V(self, state_values):\n",
    "        max_V = max(state_values.values())\n",
    "        chosen_state = random.choice([state for state, v in state_values.items() if v == max_V])\n",
    "        return chosen_state\n",
    "\n",
    "    def __argmin_V(self, state_values):\n",
    "        min_V = min(state_values.values())\n",
    "        chosen_state = random.choice([state for state, v in state_values.items() if v == min_V])\n",
    "        return chosen_state\n",
    "\n",
    "    def __random_V(self, state_values):\n",
    "        return random.choice(list(state_values.keys()))\n",
    "\n",
    "    def __reward(self, game):\n",
    "        if game.winner == self.value_player:\n",
    "            return 1.0\n",
    "        elif game.winner:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def __request_human_move(self, game):\n",
    "        allowed_moves = [i+1 for i in range(9) if game.state[i] == ' ']\n",
    "        human_move = None\n",
    "        while not human_move:\n",
    "            idx = int(input('Choose move for {}, from {} : '.format(game.player, allowed_moves)))\n",
    "            if any([i==idx for i in allowed_moves]):\n",
    "                human_move = game.state[:idx-1] + game.player + game.state[idx:]\n",
    "        return human_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_game_stats(agent):\n",
    "    results = [agent.demo_game(verbose=False) for i in range(10000)]\n",
    "    game_stats = {k: results.count(k)/100 for k in ['X', 'O', '-']}\n",
    "    print(\"    percentage results: {}\".format(game_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before learning:\n",
      "    percentage results: {'X': 58.38, 'O': 29.06, '-': 12.56}\n",
      "After 1000 learning games:\n",
      "After 5000 learning games:\n",
      "After 10000 learning games:\n",
      "After 20000 learning games:\n",
      "After 30000 learning games:\n",
      "    percentage results: {'X': 0.0, 'O': 0.0, '-': 100.0}\n",
      " \n",
      "Turn 0\n",
      "\n",
      "       |   |   \n",
      "    -----------\n",
      "       |   |   \n",
      "    -----------\n",
      "       |   |   \n",
      " \n",
      "Turn 1\n",
      "\n",
      "       |   |   \n",
      "    -----------\n",
      "       |   |   \n",
      "    -----------\n",
      "     X |   |   \n"
     ]
    }
   ],
   "source": [
    "agent = Agent(TicTacToeGame, epsilon = 0.1, alpha = 1.0)\n",
    "print(\"Before learning:\")\n",
    "demo_game_stats(agent)\n",
    "\n",
    "agent.learn_game(1000)\n",
    "print(\"After 1000 learning games:\")\n",
    "demo_game_stats(agent)\n",
    "\n",
    "agent.learn_game(4000)\n",
    "print(\"After 5000 learning games:\")\n",
    "demo_game_stats(agent)\n",
    "\n",
    "agent.learn_game(5000)\n",
    "print(\"After 10000 learning games:\")\n",
    "demo_game_stats(agent)\n",
    "\n",
    "agent.learn_game(10000)\n",
    "print(\"After 20000 learning games:\")\n",
    "demo_game_stats(agent)\n",
    "\n",
    "agent.learn_game(10000)\n",
    "print(\"After 30000 learning games:\")\n",
    "demo_game_stats(agent)\n",
    "\n",
    "agent.round_V()\n",
    "agent.save_v_table()\n",
    "agent.interactive_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Turn 0\n",
      "\n",
      "       |   |   \n",
      "    -----------\n",
      "       |   |   \n",
      "    -----------\n",
      "       |   |   \n",
      " \n",
      "Turn 1\n",
      "\n",
      "       |   |   \n",
      "    -----------\n",
      "     X |   |   \n",
      "    -----------\n",
      "       |   |   \n",
      " \n",
      "Turn 2\n",
      "\n",
      "       |   | O \n",
      "    -----------\n",
      "     X |   |   \n",
      "    -----------\n",
      "       |   |   \n",
      " \n",
      "Turn 3\n",
      "\n",
      "     X |   | O \n",
      "    -----------\n",
      "     X |   |   \n",
      "    -----------\n",
      "       |   |   \n",
      " \n",
      "Turn 4\n",
      "\n",
      "     X |   | O \n",
      "    -----------\n",
      "     X |   |   \n",
      "    -----------\n",
      "     O |   |   \n",
      " \n",
      "Turn 5\n",
      "\n",
      "     X |   | O \n",
      "    -----------\n",
      "     X | X |   \n",
      "    -----------\n",
      "     O |   |   \n",
      " \n",
      "Turn 6\n",
      "\n",
      "     X |   | O \n",
      "    -----------\n",
      "     X | X | O \n",
      "    -----------\n",
      "     O |   |   \n",
      " \n",
      "Turn 7\n",
      "\n",
      "     X |   | O \n",
      "    -----------\n",
      "     X | X | O \n",
      "    -----------\n",
      "     O |   | X \n",
      "\n",
      "X is the winner!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'X'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.interactive_game() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
